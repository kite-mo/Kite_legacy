{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kitemo/.conda/envs/kitemo_env_240708/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = torch.device('cuda:{}'.format(0) if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device) # change allocation of current GPU \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_num = 11\n",
    "sensor_list = [f'Sensor_{num}' for num in range(1, sensor_num + 1)]\n",
    "\n",
    "classes = [0, 1]\n",
    "class_probabilities = [0.9, 0.1]\n",
    "\n",
    "min_data_length, max_data_length = 90, 102\n",
    "data_cycle_num = 5\n",
    "data_num = 50\n",
    "data_key_list = ['key','cycle']\n",
    "\n",
    "dataset_df_list = []\n",
    "\n",
    "for cycle in range(1, data_cycle_num + 1):\n",
    "    for data_index in range(data_num):\n",
    "        sampling_size = random.sample(range(min_data_length, max_data_length), 1)[0]\n",
    "        class_label = random.choices(classes, class_probabilities, k=1)[0]    \n",
    "\n",
    "        generation_array = np.random.rand(sampling_size, sensor_num)\n",
    "        generation_df = pd.DataFrame(generation_array, columns=sensor_list)\n",
    "        generation_df['cycle'] = cycle\n",
    "        generation_df['key'] = data_index\n",
    "        generation_df['label'] = class_label\n",
    "        \n",
    "        dataset_df_list.append(generation_df)\n",
    "\n",
    "concat_df = pd.concat(dataset_df_list, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get cycle statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_num = concat_df.cycle.unique()\n",
    "cycle_dict = {}\n",
    "\n",
    "c_m_list = []\n",
    "c_s_list = []\n",
    "\n",
    "for cycle in cycle_num:\n",
    "    cycle_df = concat_df[concat_df.cycle == cycle]\n",
    "    wafer_unit = list(cycle_df.groupby(data_key_list)) # KEY 값\n",
    "\n",
    "    w_mean = np.array([df.loc[:, sensor_list].mean().values for info, df in wafer_unit])\n",
    "    w_std = np.array([df.loc[:, sensor_list].std().values for info, df in wafer_unit])\n",
    "\n",
    "    c_mean = np.mean(w_mean, axis = 0)\n",
    "    c_m_list.append(c_mean)\n",
    "    \n",
    "    c_std = np.mean(w_std, axis = 0)\n",
    "    c_s_list.append(c_std)\n",
    "    \n",
    "cycle_dict['mean'] = np.array(c_m_list)\n",
    "cycle_dict['std'] = np.array(c_s_list)\n",
    "\n",
    "dataset_list = list(concat_df.groupby(data_key_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetAE(Dataset):\n",
    "    def __init__(self, wafer_unit,  max_len, cycle_dict, sensor_list):\n",
    "        \n",
    "        super(datasetAE, self).__init__()\n",
    "        self.wafer_unit = wafer_unit\n",
    "        self.max_len = max_len\n",
    "        self.cycle_dict = cycle_dict\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        info = self.wafer_unit[idx][0]\n",
    "        df = self.wafer_unit[idx][1].reset_index(drop=True)\n",
    "    \n",
    "        c_mean = self.cycle_dict['mean'][0]\n",
    "        c_std = self.cycle_dict['std'][0]\n",
    "        y = df.label.unique()[0]\n",
    "        \n",
    "        sensor_df = df.loc[:, sensor_list].reset_index(drop=True)\n",
    "                \n",
    "        # padding length\n",
    "        if len(sensor_df) < self.max_len:\n",
    "            new_index = list(range(self.max_len))\n",
    "            sensor_df = sensor_df.reindex(new_index).ffill()\n",
    "                \n",
    "        sensor_array = np.array(sensor_df)\n",
    "        \n",
    "        # nomralization with cycle mean and std\n",
    "        normed_array = (sensor_array - c_mean) / (c_std)\n",
    "        \n",
    "        # (time_length, sensor_num) -> (sensor_num, time_length)\n",
    "        x = np.array(normed_array).T\n",
    "        \n",
    "        return np.array(x), np.array(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wafer_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(set([len(dataset[1]) for dataset in dataset_list]))\n",
    "\n",
    "torch_dataset = datasetAE(dataset_list, max_len, cycle_dict, sensor_list)\n",
    "torch_loader = DataLoader(torch_dataset, batch_size=64, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 101)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I_CNN(torch.nn.Module):\n",
    "    def __init__(self, sensor_num):\n",
    "        super(I_CNN, self).__init__()\n",
    "\n",
    "        layer_list = []\n",
    "        for num in range(sensor_num):\n",
    "            extract_layer = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(in_channels=1, out_channels=256, kernel_size=9, stride=2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool1d(kernel_size=3, stride =3),\n",
    "                torch.nn.Conv1d(in_channels=256, out_channels=256, kernel_size=5, stride=2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool1d(kernel_size=3, stride =3),\n",
    "            )\n",
    "\n",
    "            layer_list.append(extract_layer)\n",
    "\n",
    "        self.extract_layer = nn.ModuleList(layer_list)\n",
    "\n",
    "        self.diagnosis_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=sensor_num, out_channels=256, kernel_size=512),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.detection_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, sensor_num, sequence = x.shape\n",
    "\n",
    "        # (batch_size, sensor_num, sequence) -> (sensor_num, batch_size, sequence)\n",
    "        x_ = x.transpose(1,0)\n",
    "\n",
    "        # 센서 별로 convolution 적용        \n",
    "        stack_sensor = []\n",
    "        for sensor_num, x__ in enumerate(x_):\n",
    "            # (batch_size, sequence)\n",
    "            # Feature Extraction Layer\n",
    "            x__ = x__.unsqueeze(1)\n",
    "            feature = self.extract_layer[sensor_num](x__)\n",
    "            # (batch_size, (*feature_size))\n",
    "            flatten = feature.view(batch_size, -1)\n",
    "            # (batch_size, 1)\n",
    "            stack_sensor.append(flatten)\n",
    "\n",
    "        # Stack By Sensor\n",
    "        # (sensor_num, batch_size, out_channels_num) -> (batch_size, sensor_num, out_channels_num)\n",
    "        feature_stack = torch.stack(stack_sensor).transpose(0, 1)\n",
    "        \n",
    "        spartial_out = self.diagnosis_layer(feature_stack)\n",
    "        out = self.detection_layer(spartial_out.squeeze())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.rand(64, 11, 101)\n",
    "model = I_CNN(11)\n",
    "output = model(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name == 'diagnosis_layer.0.weight':\n",
    "        weight = param.detach().cpu().numpy()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_group_regul(model, sensor_num, alpha, mode):\n",
    "    '''\n",
    "    model : I-CNN model\n",
    "    sensor_num : number of sensors\n",
    "    alpha : weights of l1, 12 loss\n",
    "    mode : train (training mode) , valid (validation mode)\n",
    "    '''\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name == 'diagnosis_layer.0.weight':\n",
    "            weight = param\n",
    "\n",
    "    if mode == 'train':\n",
    "        norm_weights = torch.tensor(0.0, requires_grad=True)\n",
    "    elif mode == 'valid':\n",
    "        norm_weights = torch.tensor(0.0)\n",
    "\n",
    "    for sensor in range(0, sensor_num):\n",
    "        sensor_weight = weight[:, sensor, :]\n",
    "        para_m = np.sqrt(len(sensor_weight.flatten()))\n",
    "\n",
    "        # L1 part\n",
    "        l1_loss = (alpha) * torch.norm(sensor_weight, p=1)\n",
    "\n",
    "        # L2 part\n",
    "        l2_loss = (1 - alpha) * (para_m) * torch.norm(sensor_weight, p=2)\n",
    "\n",
    "        norm_weights = norm_weights + (l1_loss + l2_loss)\n",
    "\n",
    "    return norm_weights\n",
    "\n",
    "def loss_icnn(model, x, y, criterion, sensor_num, alpha = 0.5, lambda_ = 0.1, mode = 'train'):\n",
    "    '''\n",
    "    model : I-CNN model\n",
    "    x : input data\n",
    "    y : label\n",
    "    criterion : loss funcion e.g. nn.CrossEntrophy\n",
    "    sensor_num : number of sensors\n",
    "    alpha : weights of l1, 12 loss\n",
    "    mode : t (training mode) , v (validation mode)\n",
    "    lambda_ : weight of sparse_group_regul_loss\n",
    "    '''\n",
    "    \n",
    "    output = model(x)\n",
    "    loss_ = criterion(output, y)\n",
    "    sparse_group_regul_loss = sparse_group_regul(model = model, sensor_num = sensor_num,\n",
    "                                                 alpha = alpha, mode = mode)\n",
    "    loss = loss_ + (lambda_ * sparse_group_regul_loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kitemo_env_240708",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
