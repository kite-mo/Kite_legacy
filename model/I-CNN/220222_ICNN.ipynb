{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/leakage/lib/python3.7/site-packages/torch/cuda/__init__.py:104: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:{}'.format(0) if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/data/psk/leakage/ID_13.csv')\n",
    "df = df[df.cycle.apply(lambda x: x in [1,3,4,5])]\n",
    "\n",
    "col_list = ['TimeStamp', 'PM1.Gas2_Monitor', 'PM1.APC_Position',\n",
    "    'PM1.SourcePwr1_Read', 'PM1.SourcePwr2_Read', 'PM1.Temp1', 'PM1.Temp2', 'WallTemp',\n",
    "    'Port_Num', 'Process_Num', 'Wafer_Status', 'Folder_Name', 'File_Name', 'Torr','Leak', 'cycle']\n",
    "df = df.loc[:, col_list]\n",
    "\n",
    "wafer_unit = list(df.groupby(['Port_Num', 'Process_Num', 'Wafer_Status',\n",
    "                                'Folder_Name', 'File_Name', 'cycle'])) # KEY 값\n",
    "wafer_list = list(filter(lambda x: 102 > len(x[1]) > 90, wafer_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:26<00:00,  6.52s/it]\n"
     ]
    }
   ],
   "source": [
    "cycle_num = df.cycle.unique()\n",
    "cycle_dict = {}\n",
    "\n",
    "cycle_mean = {}\n",
    "cycle_std = {}\n",
    "\n",
    "c_m_list = []\n",
    "c_s_list = []\n",
    "\n",
    "for cycle in tqdm(cycle_num):\n",
    "    df_ = df[df.cycle == cycle]\n",
    "    wafer_unit_ = list(df_.groupby(['Port_Num', 'Process_Num', 'Wafer_Status',\n",
    "                                    'Folder_Name', 'File_Name', 'cycle'])) # KEY 값\n",
    "    wafer_list_ = list(filter(lambda x: 102 > len(x[1]) > 90, wafer_unit_))\n",
    "\n",
    "    w_mean = np.array([df.iloc[:, :-8].mean().values for info, df in wafer_list_])\n",
    "    w_std = np.array([df.iloc[:, :-8].std().values for info, df in wafer_list_])\n",
    "\n",
    "    c_mean = np.mean(w_mean, axis = 0)\n",
    "    cycle_mean[cycle] = c_mean\n",
    "    \n",
    "    c_std = np.mean(w_std, axis = 0)\n",
    "    cycle_std[cycle] = c_std\n",
    "    \n",
    "cycle_dict['mean'] = cycle_mean\n",
    "cycle_dict['std'] = cycle_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_, X_test, y_train, y_test = train_test_split(wafer_list, range(len(wafer_list)), test_size = 0.1, random_state=1234)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_, range(len(X_train_)), test_size = 0.1, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetAE(Dataset):\n",
    "    def __init__(self, wafer_unit, window_size, max_len, cycle_dict):\n",
    "        \n",
    "        super(datasetAE, self).__init__()\n",
    "        self.wafer_unit = wafer_unit\n",
    "        self.window_size = window_size\n",
    "        self.max_len = max_len\n",
    "        self.cycle_dict = cycle_dict\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        info = self.wafer_unit[idx][0]\n",
    "        df = self.wafer_unit[idx][1]\n",
    "        cycle = df.cycle.unique()[0]\n",
    "    \n",
    "        c_mean = self.cycle_dict['mean'][cycle]\n",
    "        c_std = self.cycle_dict['std'][cycle]\n",
    "        \n",
    "        torr = df.Torr.unique()[0]\n",
    "        y = df.Leak.unique()[0]\n",
    "        \n",
    "        df_ = df.iloc[:, 1:-8].reset_index(drop=True)\n",
    "                \n",
    "        # padding length\n",
    "        if len(df_) < self.max_len:\n",
    "            \n",
    "            add_num = abs(len(df_) - self.max_len)\n",
    "            for num in range(add_num):\n",
    "                df_.loc[len(df_)] = 0\n",
    "                \n",
    "        array_df = np.array(df_)\n",
    "        df__ = (array_df - c_mean) / (c_std)\n",
    "        \n",
    "        x = np.array(df__).T\n",
    "        \n",
    "        return x, np.array(y), torr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wafer_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(set([len(wafer[1]) for wafer in wafer_list]))\n",
    "\n",
    "train_dataset = datasetAE(X_train, 70, max_len, cycle_dict)\n",
    "valid_dataset = datasetAE(X_valid, 70, max_len, cycle_dict)\n",
    "test_dataset = datasetAE(X_test, 70, max_len, cycle_dict)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=25, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I_CNN(torch.nn.Module):\n",
    "    def __init__(self, sensor_num):\n",
    "        super(I_CNN, self).__init__()\n",
    "\n",
    "        layer_list = []\n",
    "        for num in range(sensor_num):\n",
    "            extract_layer = torch.nn.Sequential(\n",
    "                torch.nn.Conv1d(in_channels=1, out_channels=256, kernel_size=9, stride=2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool1d(kernel_size=3, stride =3),\n",
    "                torch.nn.Conv1d(in_channels=256, out_channels=256, kernel_size=5, stride=2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool1d(kernel_size=3, stride =3),\n",
    "            )\n",
    "\n",
    "            layer_list.append(extract_layer)\n",
    "\n",
    "        self.extract_layer = nn.ModuleList(layer_list)\n",
    "\n",
    "        self.diagnosis_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=sensor_num, out_channels=256, kernel_size=512),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.detection_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b, s, t = x.shape\n",
    "        x_ = x.transpose(1, 0)\n",
    "\n",
    "        stack_sensor = []\n",
    "        for idx, x__ in enumerate(x_):\n",
    "            x__ = x__.unsqueeze(1)\n",
    "            feature = self.extract_layer[idx](x__)\n",
    "            flatten = feature.view(b, -1)\n",
    "            stack_sensor.append(flatten)\n",
    "\n",
    "        feature_stack = torch.stack(stack_sensor).transpose(0, 1)\n",
    "        spartial_out = self.diagnosis_layer(feature_stack)\n",
    "        out = self.detection_layer(spartial_out.squeeze())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_group_regul(model, sensor_num, alpha, mode):\n",
    "    '''\n",
    "    model : I-CNN model\n",
    "    sensor_num : number of sensors\n",
    "    alpha : weights of l1, 12 loss\n",
    "    mode : t (training mode) , v (validation mode)\n",
    "    '''\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name == 'diagnosis_layer.0.weight':\n",
    "            weight = param\n",
    "\n",
    "    if mode == 't':\n",
    "        norm_weights = torch.tensor(0.0, requires_grad=True)\n",
    "    elif mode == 'v':\n",
    "        norm_weights = torch.tensor(0.0)\n",
    "\n",
    "    for sensor in range(0, sensor_num):\n",
    "        sensor_weight = weight[:, sensor, :]\n",
    "        para_m = np.sqrt(len(sensor_weight.flatten()))\n",
    "\n",
    "        # L1 part\n",
    "        l1_loss = (alpha) * torch.norm(sensor_weight, p=1)\n",
    "\n",
    "        # L2 part\n",
    "        l2_loss = (1 - alpha) * (para_m) * torch.norm(sensor_weight, p=2)\n",
    "\n",
    "        norm_weights = norm_weights + (l1_loss + l2_loss)\n",
    "\n",
    "    return norm_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_icnn(model, x, y, criterion, sensor_num, alpha = 0.5, lambda_ = 0.1, mode = 't'):\n",
    "    '''\n",
    "    model : I-CNN model\n",
    "    x : input data\n",
    "    y : label\n",
    "    criterion : loss funcion e.g. nn.CrossEntrophy\n",
    "    sensor_num : number of sensors\n",
    "    alpha : weights of l1, 12 loss\n",
    "    mode : t (training mode) , v (validation mode)\n",
    "    lambda_ : weight of sparse_group_regul_loss\n",
    "    '''\n",
    "    \n",
    "    output = model(x)\n",
    "    loss_ = criterion(output, y)\n",
    "    sparse_group_regul_loss = sparse_group_regul(model = model, sensor_num = sensor_num,\n",
    "                                                 alpha = alpha, mode = mode)\n",
    "    loss = loss_ + (lambda_ * sparse_group_regul_loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = I_CNN(7).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8421ac13446548876abf49d21037df16d1cb364ce87fb3bcd60f2b8aeae3feb0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('leakage')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
