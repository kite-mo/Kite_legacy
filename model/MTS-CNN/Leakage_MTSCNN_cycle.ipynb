{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3165c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:{}'.format(0) if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d846b",
   "metadata": {},
   "source": [
    "## Get 500 mtorr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd5f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/data/psk/leakage/ID_9.csv')\n",
    "df.TimeStamp = pd.to_datetime(df.TimeStamp)\n",
    "\n",
    "col_list = ['TimeStamp','PM1.Gas1_Monitor', 'PM1.Gas2_Monitor', 'PM1.APC_Pressure', 'PM1.APC_Position', \n",
    "    'PM1.SourcePwr1_Read', 'PM1.SourcePwr2_Read', 'PM1.Temp1', 'PM1.Temp2',  \n",
    "    'ApcPosition', 'ApcPositionScaled','WallTemp',\n",
    "    'Port_Num', 'Process_Num', 'Wafer_Status', 'Folder_Name', 'File_Name', 'Torr','Leak', 'cycle']\n",
    "df = df.loc[:, col_list]\n",
    "\n",
    "df_ = df[df.cycle >= 4]\n",
    "df_.loc[df_.cycle == 4, 'cycle'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab27e7",
   "metadata": {},
   "source": [
    "## get cycle info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9818721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.53s/it]\n"
     ]
    }
   ],
   "source": [
    "cycle_num = df_.cycle.unique()\n",
    "cycle_dict = {}\n",
    "\n",
    "c_m_list = []\n",
    "c_s_list = []\n",
    "\n",
    "for cycle in tqdm(cycle_num):\n",
    "    df__ = df_[df_.cycle == cycle]\n",
    "    wafer_unit = list(df__.groupby(['Port_Num', 'Process_Num', 'Wafer_Status', 'Folder_Name', 'File_Name', 'cycle'])) # KEY 값\n",
    "    wafer_list = list(filter(lambda x: 102 > len(x[1]) > 90, wafer_unit))\n",
    "\n",
    "    w_mean = np.array([df.iloc[:, 1:-8].mean().values for info, df in wafer_list])\n",
    "    w_std = np.array([df.iloc[:, 1:-8].std().values for info, df in wafer_list])\n",
    "\n",
    "    c_mean = np.mean(w_mean, axis = 0)\n",
    "    c_m_list.append(c_mean)\n",
    "    \n",
    "    c_std = np.mean(w_std, axis = 0)\n",
    "    c_s_list.append(c_std)\n",
    "    \n",
    "cycle_dict['mean'] = np.array(c_m_list)\n",
    "cycle_dict['std'] = np.array(c_s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e567d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': array([[12756.19298396,  1260.90942482,  1464.4048213 ,   678.18603717,\n",
       "          2643.8230873 ,  2639.87775034,   248.81512302,   248.66953823,\n",
       "         33554.38096869,  1470.71486598,    77.60365012]]),\n",
       " 'std': array([[1.70786988e+03, 2.03284712e+02, 2.12208593e+02, 1.89447918e+02,\n",
       "         2.16691624e+03, 2.16375136e+03, 5.07406031e-01, 4.86004885e-01,\n",
       "         7.44315737e+03, 1.87977010e+02, 2.50933011e-01]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycle_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209ddaf",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574140a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4213\n",
      "1     934\n",
      "dtype: int64\n",
      "0    478\n",
      "1     94\n",
      "dtype: int64\n",
      "0    523\n",
      "1    113\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wafer_unit = list(df_.groupby(['Port_Num', 'Process_Num', 'Wafer_Status', 'Folder_Name', 'File_Name', 'cycle'])) # KEY 값\n",
    "wafer_list = list(filter(lambda x: 102 > len(x[1]) > 90, wafer_unit))\n",
    "\n",
    "X_train_, X_test, y_train, y_test = train_test_split(wafer_list, range(len(wafer_list)), test_size = 0.1, random_state=42)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_, range(len(X_train_)), test_size = 0.1, random_state=42)\n",
    "\n",
    "split_data = [X_train, X_valid, X_test]\n",
    "\n",
    "for data in split_data:\n",
    "    print(pd.Series([wafer[1].Leak.unique()[0] for wafer in data]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a626b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a65a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetAE(Dataset):\n",
    "    def __init__(self, wafer_unit, window_size, max_len, cycle_dict):\n",
    "        \n",
    "        super(datasetAE, self).__init__()\n",
    "        self.wafer_unit = wafer_unit\n",
    "        self.window_size = window_size\n",
    "        self.max_len = max_len\n",
    "        self.cycle_dict = cycle_dict\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        info = self.wafer_unit[idx][0]\n",
    "        df = self.wafer_unit[idx][1]\n",
    "    \n",
    "        c_mean = self.cycle_dict['mean'][0]\n",
    "        c_std = self.cycle_dict['std'][0]\n",
    "        \n",
    "        torr = df.Torr.unique()[0]\n",
    "        y = df.Leak.unique()[0]\n",
    "        \n",
    "        df_ = df.iloc[:, 1:-8].reset_index(drop=True)\n",
    "                \n",
    "        # padding length\n",
    "        if len(df_) < self.max_len:\n",
    "            \n",
    "            add_num = abs(len(df_) - self.max_len)\n",
    "            for num in range(add_num):\n",
    "                df_.loc[len(df_)] = 0\n",
    "                \n",
    "        array_df = np.array(df_)\n",
    "        \n",
    "        # nomralization with cycle mean and std\n",
    "        df__ = (array_df - c_mean) / (c_std)\n",
    "        \n",
    "        x = np.array(df__).T\n",
    "        windows_x = np.lib.stride_tricks.sliding_window_view(x, self.window_size, 1)\n",
    "        \n",
    "        return np.array( windows_x ), np.array(y), torr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wafer_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea367b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(set([len(wafer[1]) for wafer in wafer_list]))\n",
    "\n",
    "train_dataset = datasetAE(X_train, 70, max_len, cycle_dict)\n",
    "valid_dataset = datasetAE(X_valid, 70, max_len, cycle_dict)\n",
    "test_dataset = datasetAE(X_test, 70, max_len, cycle_dict)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf5ac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 32, 70)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809096c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ffca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTS_CNN(torch.nn.Module):\n",
    "    \n",
    "    # 3 class 모델\n",
    "    def __init__(self, sensor_num):\n",
    "        super(MTS_CNN, self).__init__()\n",
    "        \n",
    "       \n",
    "        extract_list = []\n",
    "        diagnosis_list = []\n",
    "        for num in range(sensor_num):\n",
    "            \n",
    "            extract_layer = torch.nn.Sequential(\n",
    "                    torch.nn.Conv1d(in_channels = 32, out_channels = 16, \n",
    "                                    kernel_size = 5),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.AvgPool1d(kernel_size = 3, stride = 3),\n",
    "                    torch.nn.Conv1d(in_channels = 16, out_channels = 64, \n",
    "                                    kernel_size = 5),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.AvgPool1d(kernel_size = 3, stride = 3),\n",
    "            )\n",
    "\n",
    "            \n",
    "            diagnosis_layer = torch.nn.Sequential(\n",
    "                     torch.nn.Linear(384, 256),\n",
    "                     torch.nn.ReLU(),\n",
    "                     torch.nn.Linear(256, 1),\n",
    "                     torch.nn.ReLU(),\n",
    "            )\n",
    "            \n",
    "            extract_list.append(extract_layer)\n",
    "            diagnosis_list.append(diagnosis_layer)\n",
    "        \n",
    "        self.extract_layer = nn.ModuleList(extract_list)\n",
    "        self.diagnosis_layer = nn.ModuleList(diagnosis_list)\n",
    "        \n",
    "        self.detection_layer = torch.nn.Sequential(\n",
    "                                             torch.nn.Linear(sensor_num, 32),\n",
    "                                             torch.nn.ReLU(),\n",
    "                                             torch.nn.Linear(32, 16),\n",
    "                                             torch.nn.Dropout(0.5),\n",
    "                                             torch.nn.ReLU(),\n",
    "                                             torch.nn.Linear(16, 2)\n",
    "                                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, q, w = x.shape\n",
    "        x_ = x.transpose(1,0)\n",
    "        \n",
    "        stack_sensor = []\n",
    "        # 센서 별로 convolution 적용\n",
    "        for idx, x__ in enumerate(x_):\n",
    "            # b, q, w\n",
    "            # Feature Extraction Layer\n",
    "            feature = self.extract_layer[idx](x__)\n",
    "            # (b, 1024)\n",
    "            flatten = feature.view(b, -1)\n",
    "            # Fault Diagnosis Layer \n",
    "            diagnosis = self.diagnosis_layer[idx](flatten)\n",
    "            # b, 1\n",
    "            stack_sensor.append(diagnosis)\n",
    "        \n",
    "        # Stack By Sensor\n",
    "        fin_stack = torch.stack(stack_sensor).transpose(0,1).squeeze()\n",
    "        \n",
    "        # Fault Detection Layer\n",
    "        output = self.detection_layer(fin_stack)\n",
    "        \n",
    "        return output, fin_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989b0cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTS_CNN(\n",
       "  (extract_layer): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv1d(32, 16, kernel_size=(5,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "      (3): Conv1d(16, 64, kernel_size=(5,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "      (5): AvgPool1d(kernel_size=(3,), stride=(3,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (diagnosis_layer): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (detection_layer): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MTS_CNN(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6caaed4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0501,  0.0227],\n",
       "         [-0.0551,  0.0398],\n",
       "         [-0.0828,  0.1205],\n",
       "         [ 0.0120,  0.0379],\n",
       "         [-0.1070,  0.1609],\n",
       "         [ 0.0019,  0.0605],\n",
       "         [-0.1071,  0.1212],\n",
       "         [-0.0833, -0.0645],\n",
       "         [-0.0291,  0.1114],\n",
       "         [-0.0817,  0.0844],\n",
       "         [-0.0078, -0.0416],\n",
       "         [-0.0092,  0.1128],\n",
       "         [-0.0168,  0.1289],\n",
       "         [-0.0749, -0.0192],\n",
       "         [ 0.0722, -0.0547],\n",
       "         [-0.0238, -0.0312],\n",
       "         [-0.0587,  0.0319],\n",
       "         [ 0.0586,  0.0405],\n",
       "         [-0.0611,  0.1622],\n",
       "         [-0.1506, -0.0063],\n",
       "         [ 0.0086, -0.0371],\n",
       "         [-0.0109,  0.0339],\n",
       "         [-0.1194,  0.1180],\n",
       "         [-0.0757, -0.0212],\n",
       "         [-0.1223,  0.0488],\n",
       "         [-0.0059, -0.0046],\n",
       "         [-0.0952,  0.1045],\n",
       "         [-0.1203,  0.0219],\n",
       "         [-0.1268,  0.1253],\n",
       "         [ 0.0018,  0.0539],\n",
       "         [-0.0077,  0.0709],\n",
       "         [-0.0077,  0.1259],\n",
       "         [-0.0603,  0.0094],\n",
       "         [-0.2101,  0.0649],\n",
       "         [-0.1057,  0.1171],\n",
       "         [-0.0013,  0.0353],\n",
       "         [-0.1082,  0.1718],\n",
       "         [ 0.0254,  0.0531],\n",
       "         [-0.0889,  0.0389],\n",
       "         [-0.2159,  0.0410],\n",
       "         [ 0.0171,  0.0455],\n",
       "         [-0.0665, -0.0961],\n",
       "         [-0.0133,  0.0263],\n",
       "         [ 0.0150,  0.0767],\n",
       "         [-0.0919,  0.1073],\n",
       "         [-0.1061,  0.1741],\n",
       "         [-0.0494, -0.0637],\n",
       "         [ 0.0215,  0.1217],\n",
       "         [-0.0200,  0.0291],\n",
       "         [-0.0848,  0.0914],\n",
       "         [-0.0933,  0.0820],\n",
       "         [-0.0013,  0.0372],\n",
       "         [-0.1429,  0.0949],\n",
       "         [-0.1164,  0.0740],\n",
       "         [-0.0480, -0.0079],\n",
       "         [-0.0261,  0.0948],\n",
       "         [-0.1136, -0.0253],\n",
       "         [ 0.0212,  0.0848],\n",
       "         [-0.0130, -0.0658],\n",
       "         [-0.0816,  0.0345],\n",
       "         [ 0.0199, -0.0565],\n",
       "         [-0.0360,  0.0177],\n",
       "         [ 0.0296, -0.0454],\n",
       "         [-0.0266,  0.0678]], grad_fn=<AddmmBackward>),\n",
       " tensor([[0.0655, 0.0823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0158, 0.0715, 0.0109,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0675, 0.0779, 0.0000, 0.0000, 0.0000, 0.0000, 0.0166, 0.0704, 0.0219,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0690, 0.0779, 0.0000, 0.0000, 0.0000, 0.0000, 0.0189, 0.0654, 0.0149,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0658, 0.0816, 0.0000, 0.0000, 0.0000, 0.0000, 0.0193, 0.0716, 0.0189,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0709, 0.0820, 0.0000, 0.0000, 0.0000, 0.0000, 0.0180, 0.0671, 0.0170,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0692, 0.0797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0207, 0.0698, 0.0171,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0695, 0.0801, 0.0000, 0.0000, 0.0000, 0.0000, 0.0309, 0.0685, 0.0191,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0709, 0.0775, 0.0000, 0.0000, 0.0000, 0.0000, 0.0238, 0.0680, 0.0158,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0677, 0.0769, 0.0000, 0.0000, 0.0000, 0.0000, 0.0198, 0.0708, 0.0185,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0727, 0.0766, 0.0000, 0.0000, 0.0000, 0.0000, 0.0173, 0.0705, 0.0168,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0679, 0.0752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0197, 0.0716, 0.0154,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0690, 0.0811, 0.0000, 0.0000, 0.0000, 0.0000, 0.0225, 0.0708, 0.0134,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0652, 0.0791, 0.0000, 0.0000, 0.0000, 0.0000, 0.0249, 0.0667, 0.0164,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0675, 0.0817, 0.0000, 0.0000, 0.0000, 0.0000, 0.0170, 0.0647, 0.0146,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0698, 0.0801, 0.0000, 0.0000, 0.0000, 0.0000, 0.0197, 0.0677, 0.0240,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0689, 0.0752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0217, 0.0694, 0.0135,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0670, 0.0798, 0.0000, 0.0000, 0.0000, 0.0000, 0.0186, 0.0692, 0.0125,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0734, 0.0797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0197, 0.0645, 0.0150,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0658, 0.0781, 0.0000, 0.0000, 0.0000, 0.0000, 0.0179, 0.0680, 0.0185,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0667, 0.0790, 0.0000, 0.0000, 0.0000, 0.0000, 0.0243, 0.0699, 0.0139,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0675, 0.0782, 0.0000, 0.0000, 0.0000, 0.0000, 0.0210, 0.0698, 0.0176,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0694, 0.0806, 0.0000, 0.0000, 0.0000, 0.0000, 0.0271, 0.0675, 0.0152,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0680, 0.0819, 0.0000, 0.0000, 0.0000, 0.0000, 0.0220, 0.0694, 0.0209,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0701, 0.0802, 0.0000, 0.0000, 0.0000, 0.0000, 0.0263, 0.0635, 0.0182,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0692, 0.0790, 0.0000, 0.0000, 0.0000, 0.0000, 0.0172, 0.0702, 0.0133,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0684, 0.0799, 0.0000, 0.0000, 0.0000, 0.0000, 0.0249, 0.0702, 0.0230,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0719, 0.0790, 0.0000, 0.0000, 0.0000, 0.0000, 0.0224, 0.0653, 0.0206,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0670, 0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0171, 0.0684, 0.0128,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0714, 0.0825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0219, 0.0672, 0.0147,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0718, 0.0810, 0.0000, 0.0000, 0.0000, 0.0000, 0.0262, 0.0682, 0.0134,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0682, 0.0810, 0.0000, 0.0000, 0.0000, 0.0000, 0.0224, 0.0675, 0.0128,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0688, 0.0800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0190, 0.0697, 0.0150,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0693, 0.0818, 0.0000, 0.0000, 0.0000, 0.0000, 0.0243, 0.0687, 0.0136,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0662, 0.0756, 0.0000, 0.0000, 0.0000, 0.0000, 0.0135, 0.0677, 0.0162,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0662, 0.0782, 0.0000, 0.0000, 0.0000, 0.0000, 0.0173, 0.0664, 0.0178,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0685, 0.0764, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.0662, 0.0190,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0668, 0.0794, 0.0000, 0.0000, 0.0000, 0.0000, 0.0183, 0.0684, 0.0156,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0679, 0.0771, 0.0000, 0.0000, 0.0000, 0.0000, 0.0219, 0.0704, 0.0178,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0681, 0.0767, 0.0000, 0.0000, 0.0000, 0.0000, 0.0244, 0.0710, 0.0157,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0665, 0.0786, 0.0000, 0.0000, 0.0000, 0.0000, 0.0241, 0.0696, 0.0237,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0690, 0.0809, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.0676, 0.0144,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0716, 0.0805, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.0674, 0.0187,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0701, 0.0786, 0.0000, 0.0000, 0.0000, 0.0000, 0.0221, 0.0688, 0.0120,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0665, 0.0799, 0.0000, 0.0000, 0.0000, 0.0000, 0.0246, 0.0669, 0.0160,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0712, 0.0803, 0.0000, 0.0000, 0.0000, 0.0000, 0.0166, 0.0696, 0.0149,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0692, 0.0778, 0.0000, 0.0000, 0.0000, 0.0000, 0.0237, 0.0702, 0.0152,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0696, 0.0790, 0.0000, 0.0000, 0.0000, 0.0000, 0.0152, 0.0667, 0.0178,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0670, 0.0787, 0.0000, 0.0000, 0.0000, 0.0000, 0.0212, 0.0636, 0.0197,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0695, 0.0805, 0.0000, 0.0000, 0.0000, 0.0000, 0.0238, 0.0640, 0.0094,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0671, 0.0778, 0.0000, 0.0000, 0.0000, 0.0000, 0.0137, 0.0717, 0.0108,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0670, 0.0782, 0.0000, 0.0000, 0.0000, 0.0000, 0.0173, 0.0702, 0.0181,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0680, 0.0797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0083, 0.0691, 0.0148,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0699, 0.0816, 0.0000, 0.0000, 0.0000, 0.0000, 0.0189, 0.0706, 0.0153,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0670, 0.0790, 0.0000, 0.0000, 0.0000, 0.0000, 0.0124, 0.0688, 0.0112,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0686, 0.0764, 0.0000, 0.0000, 0.0000, 0.0000, 0.0192, 0.0676, 0.0117,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0716, 0.0807, 0.0000, 0.0000, 0.0000, 0.0000, 0.0158, 0.0697, 0.0143,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0690, 0.0830, 0.0000, 0.0000, 0.0000, 0.0000, 0.0145, 0.0707, 0.0128,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0736, 0.0821, 0.0000, 0.0000, 0.0000, 0.0000, 0.0171, 0.0630, 0.0156,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0680, 0.0767, 0.0000, 0.0000, 0.0000, 0.0000, 0.0266, 0.0666, 0.0198,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0691, 0.0794, 0.0000, 0.0000, 0.0000, 0.0000, 0.0177, 0.0634, 0.0132,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0690, 0.0780, 0.0000, 0.0000, 0.0000, 0.0000, 0.0122, 0.0675, 0.0128,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0694, 0.0791, 0.0000, 0.0000, 0.0000, 0.0000, 0.0119, 0.0634, 0.0115,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0676, 0.0809, 0.0000, 0.0000, 0.0000, 0.0000, 0.0170, 0.0674, 0.0167,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0720, 0.0810, 0.0000, 0.0000, 0.0000, 0.0000, 0.0194, 0.0680, 0.0174,\n",
       "          0.0000, 0.0000]], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.rand(64, 11, 32, 70)\n",
    "model = MTS_CNN(11)\n",
    "model(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6f00b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0715f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_model_by_date(model, optimizer, model_name, save_path, epoch, n_epochs, avg_valid_losses):\n",
    "    \n",
    "    global new \n",
    "    \n",
    "    now = datetime.now().strftime('%Y_%m_%d') \n",
    "    date_save_path = save_path + '/' + now + '/'\n",
    "    \n",
    "    if not os.path.exists(date_save_path):\n",
    "        os.makedirs(date_save_path)\n",
    "        new = 1\n",
    "        \n",
    "    if epoch == 1:\n",
    "        new = 0\n",
    "        if len(glob.glob(date_save_path + '*.pt')) > 0:\n",
    "            for ex_model in glob.glob(date_save_path + '*.pt'):\n",
    "                os.remove(ex_model)\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))\n",
    "        else:\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))\n",
    "    \n",
    "    elif new == 1:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))\n",
    "        new = 0\n",
    "    \n",
    "    elif n_epochs == epoch:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_last_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))\n",
    "    \n",
    "    else:\n",
    "        if avg_valid_losses[-1] < np.min(avg_valid_losses[:-1]):\n",
    "            print('updated model saved!')\n",
    "            ex_model = glob.glob(date_save_path + '*.pt')[0]\n",
    "            os.remove(ex_model)\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b43dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train_model(model, model_name, save_path, n_epochs, device, train_loader, valid_loader, optimizer, criterion):\n",
    "    \n",
    "    model.to(device)\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    \n",
    "    print('start-training')\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # to track the training loss as the model trains\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, label, torr) in tqdm(enumerate(train_loader, 1)):\n",
    "            data = data.float().to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output, fin_stack = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "#             print(list(model.layer_dict['0'][0].parameters())[0][0][0])\n",
    "#             print(list(model.layer_dict['0'][1].parameters())[0][0])\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        \n",
    "        print('epochs : {} / avg_train_loss : {} '.format(epoch, train_loss))\n",
    "        \n",
    "        avg_train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            for batch, (data, label, torr) in tqdm(enumerate(valid_loader, 1)):\n",
    "                data = data.float().to(device)\n",
    "                label = label.to(device)\n",
    "                output, fin_stack = model(data)\n",
    "                loss = criterion(output, label)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "            valid_loss = np.average(valid_losses)\n",
    "\n",
    "            print('epochs : {} / avg_valid_loss : {} '.format(epoch, valid_loss))\n",
    "\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "            \n",
    "        if epoch == 1:\n",
    "            pass\n",
    "        else:\n",
    "            if avg_valid_losses[-1] < np.min(avg_valid_losses[:-1]):\n",
    "                print('updated model saved!')\n",
    "                model_info = [epoch, model.state_dict()]\n",
    "                \n",
    "        save_model_by_date(model, optimizer, model_name, save_path, epoch, n_epochs, avg_valid_losses)\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses, model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b54da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MTS_CNN(11).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "n_epochs = 200\n",
    "model_name = 'CLF'\n",
    "save_path = './Leakage_model/MTS-CNN_Cycle'\n",
    "\n",
    "model, avg_train_losses, avg_valid_losses, model_info = train_model(model, model_name, save_path, n_epochs, \n",
    "                                                                    device, train_loader, valid_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5cf5b",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1bb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MTS_CNN(11).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "best_model_path = './Leakage_model/MTS-CNN_Cycle/2021_11_17'\n",
    "model_pt = glob.glob(best_model_path + '/*')[0]\n",
    "print(model_pt)\n",
    "\n",
    "checkpoint = torch.load(model_pt)\n",
    "model.load_state_dict((checkpoint['model_state_dict']))\n",
    "print('<All keys matched successfully>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeae887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test_model(model, test_dl, device):\n",
    "\n",
    "    target_list = []\n",
    "    predict_list = []\n",
    "    dia_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for num, data in enumerate(tqdm(test_dl)):\n",
    "\n",
    "            data, labels, torr = data\n",
    "\n",
    "            data = data.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs, fin_stack= model(data)\n",
    "            predicted = torch.argmax(outputs.data)\n",
    "            \n",
    "            target_list.append(labels.cpu().numpy())\n",
    "            predict_list.append(predicted.cpu().numpy())\n",
    "\n",
    "            dia_list.append(fin_stack.cpu())\n",
    "\n",
    "        fin_target = np.concatenate(target_list, axis = 0)\n",
    "        fin_pred = np.array(predict_list)\n",
    "        fin_dia = torch.stack(dia_list)\n",
    "                \n",
    "        print(confusion_matrix(fin_target, fin_pred))\n",
    "        print(classification_report(fin_target, fin_pred))\n",
    "        \n",
    "    return fin_target, fin_pred, np.array(fin_dia.view(-1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c06a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_target, fin_pred, fin_dia = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d2ca3",
   "metadata": {},
   "source": [
    "## Get TSNE graph of fault diagnosis outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d3ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "embedded = TSNE(n_components=2, learning_rate= 200).fit_transform(fin_dia)\n",
    "fin_embed = np.concatenate([embedded, fin_target.reshape(-1, 1)] ,axis = 1)\n",
    "df_embed = pd.DataFrame(fin_embed, columns = ['x', 'y', 'label'])\n",
    "\n",
    "plt.figure(figsize = [15, 10])\n",
    "sns.scatterplot(data = df_embed, hue = 'label', x = 'x', y = 'y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1e552",
   "metadata": {},
   "source": [
    "## Get sensor importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb66d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name == 'detection_layer.0.weight':\n",
    "        weight = param\n",
    "\n",
    "weights = weight.detach().cpu().numpy()\n",
    "weights_ = weights.reshape(11, -1)\n",
    "\n",
    "col_list = ['PM1.Gas1_Monitor', 'PM1.Gas2_Monitor', 'PM1.APC_Pressure', 'PM1.APC_Position', \n",
    "    'PM1.SourcePwr1_Read', 'PM1.SourcePwr2_Read', 'PM1.Temp1', 'PM1.Temp2',  \n",
    "    'ApcPosition', 'ApcPositionScaled','WallTemp']\n",
    "\n",
    "plt.figure(figsize = [15, 10])\n",
    "box_list = []\n",
    "for w in weights_:\n",
    "    box_list.append(w)\n",
    "    \n",
    "plt.boxplot(box_list)\n",
    "plt.xticks(range(1, 12), col_list,  rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a3a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leakage",
   "language": "python",
   "name": "leakage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
