{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87145994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# device number 설정\n",
    "device = torch.device('cuda:{}'.format(0) if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5bd835",
   "metadata": {},
   "source": [
    "## 500 mtorr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3efc4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5214\n",
      "1    1141\n",
      "dtype: int64\n",
      "{99, 101}\n",
      "101    6354\n",
      "99        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/data/psk/leakage/ID_9.csv')\n",
    "df = df.dropna(axis = 0)\n",
    "df = df[df.Torr == 500]\n",
    "\n",
    "df.TimeStamp = pd.to_datetime(df.TimeStamp)\n",
    "df = df.sort_values(by = ['TimeStamp'])\n",
    "col_list = ['TimeStamp','PM1.Gas1_Monitor', 'PM1.Gas2_Monitor', 'PM1.APC_Pressure', 'PM1.APC_Position', \n",
    "    'PM1.SourcePwr1_Read', 'PM1.SourcePwr2_Read', 'PM1.Temp1', 'PM1.Temp2',  \n",
    "    'ApcPosition', 'ApcPositionScaled','WallTemp',\n",
    "    'Port_Num', 'Process_Num', 'Wafer_Status', 'Folder_Name', 'File_Name', 'Torr','Leak']\n",
    "df_ = df.loc[:, col_list]\n",
    "\n",
    "wafer_unit = list(df_.groupby(['Port_Num', 'Process_Num', 'Wafer_Status', 'Folder_Name', 'File_Name'])) # KEY 값\n",
    "wafer_list = list(filter(lambda x: 102 > len(x[1]) > 90, wafer_unit))    \n",
    "\n",
    "print(pd.Series([wafer[1].Leak.unique()[0] for wafer in wafer_list]).value_counts())\n",
    "print(set([len(wafer[1]) for wafer in wafer_list]))\n",
    "print(pd.Series([len(wafer[1]) for wafer in wafer_list]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2484f",
   "metadata": {},
   "source": [
    "## Split train, test, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1930c78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4213\n",
      "1     934\n",
      "dtype: int64\n",
      "0    478\n",
      "1     94\n",
      "dtype: int64\n",
      "0    523\n",
      "1    113\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_, X_test, y_train, y_test = train_test_split(wafer_list, range(len(wafer_list)), test_size = 0.1, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_, range(len(X_train_)), test_size = 0.1, random_state=42)\n",
    "\n",
    "split_data = [X_train, X_valid, X_test]\n",
    "\n",
    "for data in split_data:\n",
    "    print(pd.Series([wafer[1].Leak.unique()[0] for wafer in data]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c9c0e",
   "metadata": {},
   "source": [
    "## Dataset with mean std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16a23d",
   "metadata": {},
   "source": [
    "## Traing 기반 MinMaxScaler 적용\n",
    "### 각 데이터 셋에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90213d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:07,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mean_std = []\n",
    "datasets = [X_train, X_valid, X_test]\n",
    "\n",
    "for idx, dataset in tqdm(enumerate(datasets)):\n",
    "    \n",
    "    sam = []\n",
    "    \n",
    "    if idx == 0:\n",
    "        \n",
    "        m_scaler = MinMaxScaler()\n",
    "        # 해당 dataset 의 mean 과 std 구하기\n",
    "        mean = np.array([df.iloc[:, 1:-7].mean().values for info, df in dataset])\n",
    "        scale_mean = m_scaler.fit_transform(mean)\n",
    "        sam.append(scale_mean)\n",
    "\n",
    "        s_scaler = MinMaxScaler() \n",
    "        std = np.array([df.iloc[:, 1:-7].std().values for info, df in dataset])\n",
    "        scale_std = s_scaler.fit_transform(std)\n",
    "        sam.append(scale_std)\n",
    "        mean_std.append(sam)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        mean = np.array([df.iloc[:, 1:-7].mean().values for info, df in dataset])\n",
    "        scale_mean = m_scaler.transform(mean)\n",
    "        sam.append(scale_mean)\n",
    "        \n",
    "        std = np.array([df.iloc[:, 1:-7].std().values for info, df in dataset])\n",
    "        scale_std = s_scaler.transform(std)\n",
    "        sam.append(scale_std)\n",
    "\n",
    "        mean_std.append(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dec161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class datasetAE(Dataset):\n",
    "    def __init__(self, wafer_unit, window_size, max_len, mean_arr, std_arr):\n",
    "        \n",
    "        super(datasetAE, self).__init__()\n",
    "        self.wafer_unit = wafer_unit\n",
    "        self.window_size = window_size\n",
    "        self.max_len = max_len\n",
    "        self.mean_arr = mean_arr\n",
    "        self.std_arr = std_arr\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        df = self.wafer_unit[idx][1]\n",
    "        info = self.wafer_unit[idx][0]\n",
    "        torr = df.Torr.unique()[0]\n",
    "        y = df.Leak.unique()[0]\n",
    "        \n",
    "        df_ = df.iloc[:, 1:-7].reset_index(drop=True)\n",
    "        \n",
    "        mean_ = self.mean_arr[idx]\n",
    "        std_ = self.std_arr[idx]\n",
    "        \n",
    "        # zero padding\n",
    "        if len(df_) < self.max_len:\n",
    "            \n",
    "            add_num = abs(len(df_) - self.max_len)\n",
    "            for num in range(add_num):\n",
    "                df_.loc[len(df_)] = 0\n",
    "        \n",
    "        scaler = StandardScaler()                              \n",
    "        df__ = pd.DataFrame(scaler.fit_transform(df_), columns = df_.columns).reset_index(drop=-True)\n",
    "                \n",
    "        x = np.array(df__).T\n",
    "        \n",
    "        # sliding window 적용\n",
    "        windows_x = np.lib.stride_tricks.sliding_window_view(x, self.window_size, 1)\n",
    "        \n",
    "        sensor_list = []\n",
    "        for idx, sensor in enumerate(windows_x):\n",
    "            add_mean = np.full(self.window_size, mean_[idx])\n",
    "            add_std = np.full(self.window_size, std_[idx])\n",
    "            sensor = np.vstack([sensor, add_mean, add_std])\n",
    "            sensor_list.append(sensor)\n",
    "        \n",
    "        return np.array(sensor_list), np.array(y), torr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wafer_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29fac17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_std[0] : train_X, [1] : Valid_X, [2] : Test_X \n",
    "## mean_std[0][1] : train_X 의 센서별 scaled mean 값\n",
    "## mean_std[0][2] : train_X 의 센서별 scaled std 값\n",
    "\n",
    "max_len = max(set([len(wafer[1]) for wafer in wafer_list]))\n",
    "train_dataset = datasetAE(X_train, 70, max_len, mean_std[0][0], mean_std[0][1])\n",
    "valid_dataset = datasetAE(X_valid, 70, max_len, mean_std[1][0], mean_std[1][1])\n",
    "test_dataset = datasetAE(X_test, 70, max_len, mean_std[2][0], mean_std[2][1])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f572d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44706174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTS_CNN(torch.nn.Module):\n",
    "    \n",
    "    # 3 class 모델\n",
    "    def __init__(self, sensor_num):\n",
    "        super(MTS_CNN, self).__init__()\n",
    "        \n",
    "        extract_list = []\n",
    "        diagnosis_list = []\n",
    "        for num in range(sensor_num):\n",
    "            \n",
    "            extract_layer = torch.nn.Sequential(\n",
    "                    torch.nn.Conv1d(in_channels = 32, out_channels = 16, \n",
    "                                    kernel_size = 5),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.AvgPool1d(kernel_size = 3, stride = 3),\n",
    "                    torch.nn.Conv1d(in_channels = 16, out_channels = 64, \n",
    "                                    kernel_size = 5),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.AvgPool1d(kernel_size = 3, stride = 3),\n",
    "            )\n",
    "\n",
    "            \n",
    "            diagnosis_layer = torch.nn.Sequential(\n",
    "                     torch.nn.Linear(386, 256),\n",
    "                     torch.nn.ReLU(),\n",
    "                     torch.nn.Linear(256, 1),\n",
    "                     torch.nn.ReLU(),\n",
    "            )\n",
    "            \n",
    "            extract_list.append(extract_layer)\n",
    "            diagnosis_list.append(diagnosis_layer)\n",
    "            \n",
    "        \n",
    "        self.extract_layer = nn.ModuleList(extract_list)\n",
    "        self.diagnosis_layer = nn.ModuleList(diagnosis_list)\n",
    "        \n",
    "        self.detection_layer = torch.nn.Sequential(\n",
    "                                             torch.nn.Linear(sensor_num, 32),\n",
    "                                             torch.nn.ReLU(),\n",
    "                                             torch.nn.Linear(32, 16),\n",
    "                                             torch.nn.Dropout(0.5),\n",
    "                                             torch.nn.ReLU(),\n",
    "                                             torch.nn.Linear(16, 2)\n",
    "                                    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, s, q, w = x.shape\n",
    "        # s, b, q, w\n",
    "        x_ = x.transpose(1,0)\n",
    "        \n",
    "        stack_sensor = []\n",
    "        # 센서 별로 convolution 적용\n",
    "        for idx, x__ in enumerate(x_):\n",
    "            \n",
    "            # seq 만 뺴오기\n",
    "            # 32 \n",
    "            seq = x__[:, :-2, :]\n",
    "            \n",
    "            # mean, std, 분리\n",
    "            mean = x__[:, -2, 0].view(-1,1)\n",
    "            std = x__[:, -1, 0].view(-1,1)\n",
    "            \n",
    "            # Feature Extraction Layer\n",
    "            feature = self.extract_layer[idx](seq)\n",
    "            flatten = feature.view(b, -1)\n",
    "            \n",
    "            # 1024 (shape) + 2 (mean, std) = 1026\n",
    "            flatten_ = torch.cat((flatten, mean, std), 1)\n",
    "            \n",
    "            # Fault Diagnosis Layer\n",
    "            diagnosis = self.diagnosis_layer[idx](flatten_)\n",
    "            stack_sensor.append(diagnosis)\n",
    "        \n",
    "        # Stack By Sensor\n",
    "        fin_stack = torch.stack(stack_sensor).transpose(0,1).flatten(start_dim = -2)\n",
    "        \n",
    "        # Fault Detection Layer\n",
    "        output = self.detection_layer(fin_stack)\n",
    "        \n",
    "        return output, fin_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a745ed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3107,  0.1192],\n",
       "         [-0.2952,  0.0900],\n",
       "         [-0.3613,  0.0688],\n",
       "         [-0.3578,  0.0624],\n",
       "         [-0.3150,  0.1078],\n",
       "         [-0.2541,  0.1270],\n",
       "         [-0.3024,  0.0884],\n",
       "         [-0.3007,  0.1004],\n",
       "         [-0.2972,  0.1014],\n",
       "         [-0.2318,  0.1342],\n",
       "         [-0.2542,  0.1258],\n",
       "         [-0.3012,  0.0867],\n",
       "         [-0.3158,  0.1009],\n",
       "         [-0.3184,  0.0990],\n",
       "         [-0.3328,  0.1098],\n",
       "         [-0.2322,  0.1350],\n",
       "         [-0.3285,  0.1095],\n",
       "         [-0.2181,  0.1213],\n",
       "         [-0.3180,  0.1016],\n",
       "         [-0.2332,  0.1201],\n",
       "         [-0.2231,  0.1233],\n",
       "         [-0.2343,  0.1273],\n",
       "         [-0.2464,  0.1336],\n",
       "         [-0.2328,  0.1192],\n",
       "         [-0.2852,  0.0957],\n",
       "         [-0.2549,  0.1253],\n",
       "         [-0.2585,  0.1259],\n",
       "         [-0.1746,  0.1522],\n",
       "         [-0.1989,  0.1522],\n",
       "         [-0.2528,  0.1169],\n",
       "         [-0.2687,  0.1295],\n",
       "         [-0.2342,  0.1339],\n",
       "         [-0.1941,  0.1498],\n",
       "         [-0.2994,  0.1075],\n",
       "         [-0.1939,  0.1637],\n",
       "         [-0.3578,  0.0687],\n",
       "         [-0.3189,  0.1076],\n",
       "         [-0.1979,  0.1499],\n",
       "         [-0.2855,  0.1018],\n",
       "         [-0.3097,  0.0933],\n",
       "         [-0.1935,  0.1576],\n",
       "         [-0.2377,  0.1268],\n",
       "         [-0.3631,  0.0640],\n",
       "         [-0.3091,  0.1130],\n",
       "         [-0.2755,  0.0988],\n",
       "         [-0.3771,  0.0673],\n",
       "         [-0.2182,  0.1150],\n",
       "         [-0.2620,  0.1310],\n",
       "         [-0.2890,  0.0942],\n",
       "         [-0.2393,  0.1342],\n",
       "         [-0.3395,  0.0787],\n",
       "         [-0.2893,  0.0944],\n",
       "         [-0.2435,  0.1385],\n",
       "         [-0.3022,  0.0886],\n",
       "         [-0.3621,  0.0638],\n",
       "         [-0.2585,  0.1258],\n",
       "         [-0.1931,  0.1623],\n",
       "         [-0.2648,  0.1289],\n",
       "         [-0.2989,  0.1083],\n",
       "         [-0.2302,  0.1274],\n",
       "         [-0.2556,  0.1166],\n",
       "         [-0.2435,  0.1341],\n",
       "         [-0.2581,  0.1242],\n",
       "         [-0.3185,  0.1008]], grad_fn=<AddmmBackward>),\n",
       " tensor([[0.0000, 0.0146, 0.0527, 0.0307, 0.0000, 0.0424, 0.0636, 0.0000, 0.0419,\n",
       "          0.0240, 0.0848],\n",
       "         [0.0000, 0.0181, 0.0549, 0.0199, 0.0000, 0.0408, 0.0535, 0.0000, 0.0448,\n",
       "          0.0304, 0.0803],\n",
       "         [0.0000, 0.0178, 0.0600, 0.0125, 0.0000, 0.0419, 0.0507, 0.0000, 0.0485,\n",
       "          0.0212, 0.0959],\n",
       "         [0.0000, 0.0175, 0.0528, 0.0217, 0.0000, 0.0363, 0.0490, 0.0000, 0.0478,\n",
       "          0.0408, 0.0863],\n",
       "         [0.0000, 0.0133, 0.0566, 0.0196, 0.0000, 0.0401, 0.0481, 0.0000, 0.0453,\n",
       "          0.0265, 0.0901],\n",
       "         [0.0000, 0.0172, 0.0574, 0.0225, 0.0000, 0.0350, 0.0530, 0.0000, 0.0392,\n",
       "          0.0327, 0.0893],\n",
       "         [0.0000, 0.0164, 0.0536, 0.0211, 0.0000, 0.0435, 0.0480, 0.0000, 0.0360,\n",
       "          0.0299, 0.0908],\n",
       "         [0.0000, 0.0128, 0.0509, 0.0305, 0.0000, 0.0405, 0.0539, 0.0000, 0.0460,\n",
       "          0.0293, 0.0915],\n",
       "         [0.0000, 0.0216, 0.0484, 0.0196, 0.0000, 0.0454, 0.0550, 0.0000, 0.0452,\n",
       "          0.0287, 0.0865],\n",
       "         [0.0000, 0.0153, 0.0524, 0.0168, 0.0000, 0.0497, 0.0618, 0.0000, 0.0447,\n",
       "          0.0328, 0.0907],\n",
       "         [0.0000, 0.0132, 0.0576, 0.0237, 0.0000, 0.0394, 0.0629, 0.0000, 0.0381,\n",
       "          0.0252, 0.0842],\n",
       "         [0.0000, 0.0168, 0.0595, 0.0211, 0.0000, 0.0419, 0.0572, 0.0000, 0.0492,\n",
       "          0.0245, 0.0902],\n",
       "         [0.0000, 0.0103, 0.0498, 0.0195, 0.0000, 0.0430, 0.0649, 0.0000, 0.0459,\n",
       "          0.0246, 0.0850],\n",
       "         [0.0000, 0.0115, 0.0573, 0.0175, 0.0000, 0.0364, 0.0663, 0.0000, 0.0546,\n",
       "          0.0235, 0.0911],\n",
       "         [0.0000, 0.0170, 0.0584, 0.0205, 0.0000, 0.0389, 0.0612, 0.0000, 0.0482,\n",
       "          0.0260, 0.0882],\n",
       "         [0.0000, 0.0177, 0.0523, 0.0195, 0.0000, 0.0422, 0.0630, 0.0000, 0.0342,\n",
       "          0.0320, 0.0905],\n",
       "         [0.0000, 0.0147, 0.0534, 0.0144, 0.0000, 0.0386, 0.0562, 0.0000, 0.0401,\n",
       "          0.0371, 0.0879],\n",
       "         [0.0000, 0.0182, 0.0549, 0.0207, 0.0000, 0.0384, 0.0498, 0.0000, 0.0512,\n",
       "          0.0258, 0.0856],\n",
       "         [0.0000, 0.0204, 0.0575, 0.0207, 0.0000, 0.0446, 0.0458, 0.0000, 0.0395,\n",
       "          0.0282, 0.0858],\n",
       "         [0.0000, 0.0187, 0.0496, 0.0192, 0.0000, 0.0433, 0.0567, 0.0000, 0.0550,\n",
       "          0.0251, 0.0890],\n",
       "         [0.0000, 0.0208, 0.0572, 0.0257, 0.0000, 0.0369, 0.0546, 0.0000, 0.0407,\n",
       "          0.0279, 0.0986],\n",
       "         [0.0000, 0.0197, 0.0579, 0.0130, 0.0000, 0.0422, 0.0463, 0.0000, 0.0476,\n",
       "          0.0298, 0.0840],\n",
       "         [0.0000, 0.0169, 0.0573, 0.0083, 0.0000, 0.0357, 0.0638, 0.0000, 0.0433,\n",
       "          0.0245, 0.0876],\n",
       "         [0.0000, 0.0166, 0.0540, 0.0156, 0.0000, 0.0406, 0.0622, 0.0000, 0.0414,\n",
       "          0.0250, 0.0911],\n",
       "         [0.0000, 0.0198, 0.0527, 0.0198, 0.0000, 0.0453, 0.0519, 0.0000, 0.0459,\n",
       "          0.0195, 0.0886],\n",
       "         [0.0000, 0.0161, 0.0521, 0.0183, 0.0000, 0.0384, 0.0580, 0.0000, 0.0483,\n",
       "          0.0225, 0.0861],\n",
       "         [0.0000, 0.0229, 0.0518, 0.0221, 0.0000, 0.0446, 0.0452, 0.0000, 0.0489,\n",
       "          0.0294, 0.0911],\n",
       "         [0.0000, 0.0200, 0.0552, 0.0175, 0.0000, 0.0470, 0.0523, 0.0000, 0.0510,\n",
       "          0.0265, 0.0974],\n",
       "         [0.0000, 0.0205, 0.0497, 0.0241, 0.0000, 0.0277, 0.0592, 0.0000, 0.0418,\n",
       "          0.0276, 0.0896],\n",
       "         [0.0000, 0.0137, 0.0541, 0.0200, 0.0000, 0.0411, 0.0502, 0.0000, 0.0439,\n",
       "          0.0315, 0.0843],\n",
       "         [0.0000, 0.0139, 0.0568, 0.0131, 0.0000, 0.0355, 0.0564, 0.0000, 0.0423,\n",
       "          0.0242, 0.0902],\n",
       "         [0.0000, 0.0119, 0.0520, 0.0208, 0.0000, 0.0433, 0.0415, 0.0000, 0.0554,\n",
       "          0.0269, 0.0897],\n",
       "         [0.0000, 0.0214, 0.0488, 0.0171, 0.0000, 0.0312, 0.0405, 0.0000, 0.0492,\n",
       "          0.0351, 0.0897],\n",
       "         [0.0000, 0.0114, 0.0518, 0.0222, 0.0000, 0.0356, 0.0637, 0.0000, 0.0470,\n",
       "          0.0275, 0.0933],\n",
       "         [0.0000, 0.0119, 0.0538, 0.0246, 0.0000, 0.0417, 0.0559, 0.0000, 0.0382,\n",
       "          0.0284, 0.0869],\n",
       "         [0.0000, 0.0172, 0.0553, 0.0169, 0.0000, 0.0315, 0.0477, 0.0000, 0.0501,\n",
       "          0.0324, 0.0809],\n",
       "         [0.0000, 0.0159, 0.0536, 0.0207, 0.0000, 0.0364, 0.0518, 0.0000, 0.0491,\n",
       "          0.0323, 0.0916],\n",
       "         [0.0000, 0.0178, 0.0528, 0.0256, 0.0000, 0.0350, 0.0587, 0.0000, 0.0486,\n",
       "          0.0270, 0.0866],\n",
       "         [0.0000, 0.0214, 0.0543, 0.0081, 0.0000, 0.0384, 0.0598, 0.0000, 0.0491,\n",
       "          0.0350, 0.0842],\n",
       "         [0.0000, 0.0206, 0.0548, 0.0156, 0.0000, 0.0403, 0.0616, 0.0000, 0.0471,\n",
       "          0.0221, 0.0889],\n",
       "         [0.0000, 0.0167, 0.0601, 0.0231, 0.0000, 0.0441, 0.0429, 0.0000, 0.0456,\n",
       "          0.0249, 0.0885],\n",
       "         [0.0000, 0.0139, 0.0611, 0.0182, 0.0000, 0.0408, 0.0630, 0.0000, 0.0511,\n",
       "          0.0217, 0.0991],\n",
       "         [0.0000, 0.0127, 0.0503, 0.0219, 0.0000, 0.0421, 0.0609, 0.0000, 0.0488,\n",
       "          0.0146, 0.0855],\n",
       "         [0.0000, 0.0157, 0.0580, 0.0339, 0.0000, 0.0445, 0.0534, 0.0000, 0.0465,\n",
       "          0.0272, 0.0929],\n",
       "         [0.0000, 0.0181, 0.0546, 0.0134, 0.0000, 0.0364, 0.0496, 0.0000, 0.0442,\n",
       "          0.0285, 0.0835],\n",
       "         [0.0000, 0.0114, 0.0545, 0.0177, 0.0000, 0.0442, 0.0551, 0.0000, 0.0539,\n",
       "          0.0228, 0.0851],\n",
       "         [0.0000, 0.0197, 0.0517, 0.0289, 0.0000, 0.0356, 0.0593, 0.0000, 0.0475,\n",
       "          0.0285, 0.0923],\n",
       "         [0.0000, 0.0144, 0.0565, 0.0235, 0.0000, 0.0470, 0.0469, 0.0000, 0.0389,\n",
       "          0.0352, 0.0823],\n",
       "         [0.0000, 0.0136, 0.0566, 0.0235, 0.0000, 0.0422, 0.0684, 0.0000, 0.0439,\n",
       "          0.0275, 0.0925],\n",
       "         [0.0000, 0.0165, 0.0505, 0.0169, 0.0000, 0.0424, 0.0502, 0.0000, 0.0418,\n",
       "          0.0261, 0.0885],\n",
       "         [0.0000, 0.0152, 0.0537, 0.0258, 0.0000, 0.0522, 0.0604, 0.0000, 0.0434,\n",
       "          0.0314, 0.0867],\n",
       "         [0.0000, 0.0185, 0.0546, 0.0234, 0.0000, 0.0368, 0.0625, 0.0000, 0.0410,\n",
       "          0.0213, 0.0966],\n",
       "         [0.0000, 0.0149, 0.0561, 0.0224, 0.0000, 0.0314, 0.0636, 0.0000, 0.0360,\n",
       "          0.0258, 0.0894],\n",
       "         [0.0000, 0.0173, 0.0564, 0.0244, 0.0000, 0.0386, 0.0613, 0.0000, 0.0476,\n",
       "          0.0365, 0.0906],\n",
       "         [0.0000, 0.0205, 0.0483, 0.0178, 0.0000, 0.0396, 0.0517, 0.0000, 0.0437,\n",
       "          0.0248, 0.0846],\n",
       "         [0.0000, 0.0158, 0.0536, 0.0124, 0.0000, 0.0396, 0.0574, 0.0000, 0.0503,\n",
       "          0.0252, 0.0947],\n",
       "         [0.0000, 0.0227, 0.0573, 0.0203, 0.0000, 0.0484, 0.0680, 0.0000, 0.0422,\n",
       "          0.0249, 0.0873],\n",
       "         [0.0000, 0.0149, 0.0544, 0.0239, 0.0000, 0.0377, 0.0622, 0.0000, 0.0388,\n",
       "          0.0243, 0.0859],\n",
       "         [0.0000, 0.0157, 0.0540, 0.0259, 0.0000, 0.0318, 0.0599, 0.0000, 0.0542,\n",
       "          0.0297, 0.0863],\n",
       "         [0.0000, 0.0136, 0.0563, 0.0152, 0.0000, 0.0478, 0.0451, 0.0000, 0.0505,\n",
       "          0.0327, 0.0891],\n",
       "         [0.0000, 0.0141, 0.0530, 0.0128, 0.0000, 0.0491, 0.0644, 0.0000, 0.0387,\n",
       "          0.0283, 0.0860],\n",
       "         [0.0000, 0.0165, 0.0580, 0.0215, 0.0000, 0.0365, 0.0621, 0.0000, 0.0406,\n",
       "          0.0256, 0.0940],\n",
       "         [0.0000, 0.0189, 0.0551, 0.0254, 0.0000, 0.0401, 0.0564, 0.0000, 0.0376,\n",
       "          0.0288, 0.0871],\n",
       "         [0.0000, 0.0171, 0.0605, 0.0227, 0.0000, 0.0408, 0.0614, 0.0000, 0.0472,\n",
       "          0.0247, 0.0918]], grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.rand(64, 11, 34, 70) # b, s, q, w\n",
    "model = MTS_CNN(11)\n",
    "model(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8e267",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a992433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_model_by_date(model, optimizer, model_name, save_path, epoch, n_epochs, avg_valid_losses):\n",
    "    \n",
    "    global new \n",
    "    \n",
    "    now = datetime.now().strftime('%Y_%m_%d') \n",
    "    date_save_path = save_path + '/' + now + '/'\n",
    "    \n",
    "    if not os.path.exists(date_save_path):\n",
    "        os.makedirs(date_save_path)\n",
    "        new = 1\n",
    "        \n",
    "    if epoch == 1:\n",
    "        new = 0\n",
    "        if len(glob.glob(date_save_path + '*.pt')) > 0:\n",
    "            for ex_model in glob.glob(date_save_path + '*.pt'):\n",
    "                os.remove(ex_model)\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))\n",
    "        else:\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))\n",
    "    \n",
    "    elif new == 1:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))\n",
    "        new = 0\n",
    "    \n",
    "    elif n_epochs == epoch:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_last_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))\n",
    "    \n",
    "    else:\n",
    "        if avg_valid_losses[-1] < np.min(avg_valid_losses[:-1]):\n",
    "            print('updated model saved!')\n",
    "            ex_model = glob.glob(date_save_path + '*.pt')[0]\n",
    "            os.remove(ex_model)\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(date_save_path, '{}_epoch_{}_valid_loss_{}.pt'.format(model_name, str(epoch), str(avg_valid_losses[-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94efb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train_model(model, model_name, save_path, n_epochs, device, train_loader, valid_loader, optimizer, criterion):\n",
    "    \n",
    "    model.to(device)\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    \n",
    "    print('start-training')\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # to track the training loss as the model trains\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, label, torr) in tqdm(enumerate(train_loader, 1)):\n",
    "            data = data.float().to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output, fin_stack = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "#             print(list(model.layer_dict['0'][0].parameters())[0][0][0])\n",
    "#             print(list(model.layer_dict['0'][1].parameters())[0][0])\n",
    "            \n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        \n",
    "        print('epochs : {} / avg_train_loss : {} '.format(epoch, train_loss))\n",
    "        \n",
    "        avg_train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            for batch, (data, label, torr) in tqdm(enumerate(valid_loader, 1)):\n",
    "                data = data.float().to(device)\n",
    "                label = label.to(device)\n",
    "                output, fin_stack = model(data)\n",
    "                loss = criterion(output, label)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "            valid_loss = np.average(valid_losses)\n",
    "\n",
    "            print('epochs : {} / avg_valid_loss : {} '.format(epoch, valid_loss))\n",
    "\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "            \n",
    "        if epoch == 1:\n",
    "            pass\n",
    "        else:\n",
    "            if avg_valid_losses[-1] < np.min(avg_valid_losses[:-1]):\n",
    "                print('updated model saved!')\n",
    "                model_info = [epoch, model.state_dict()]\n",
    "                \n",
    "        save_model_by_date(model, optimizer, model_name, save_path, epoch, n_epochs, avg_valid_losses)\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses, model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e76c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MTS_CNN(11).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "n_epochs = 200\n",
    "# device = 0\n",
    "model_name = 'CLF'\n",
    "save_path = './Leakage_model/MTS-CNN_MS' # 저장 장소 + 오늘 날짜 저장\n",
    "\n",
    "model, avg_train_losses, avg_valid_losses, model_info = train_model(model, model_name, save_path, n_epochs, \n",
    "                                                                    device, train_loader, valid_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adf20b",
   "metadata": {},
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89273872",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MTS_CNN(11).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "best_model_path = './Leakage_model/MTS-CNN_MS/2021_11_16' # 모델 저장된 경로 입력\n",
    "model_pt = glob.glob(best_model_path + '/*')[-1]\n",
    "print(model_pt)\n",
    "\n",
    "checkpoint = torch.load(model_pt)\n",
    "model.load_state_dict((checkpoint['model_state_dict']))\n",
    "print('<All keys matched successfully>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test_model(model, test_dl, device):\n",
    "\n",
    "    target_list = []\n",
    "    predict_list = []\n",
    "    dia_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for num, data in enumerate(tqdm(test_dl)):\n",
    "\n",
    "            data, labels, torr = data\n",
    "\n",
    "            data = data.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs, fin_stack= model(data)\n",
    "            predicted = torch.argmax(outputs.data, axis = 0 )\n",
    "            \n",
    "            target_list.append(labels.cpu().numpy())\n",
    "            predict_list.append(predicted.cpu().numpy())\n",
    "\n",
    "            dia_list.append(fin_stack.cpu())\n",
    "\n",
    "        fin_target = np.concatenate(target_list, axis = 0)\n",
    "        fin_pred = np.array(predict_list)\n",
    "        fin_dia = torch.stack(dia_list)\n",
    "                \n",
    "        print(confusion_matrix(fin_target, fin_pred))\n",
    "        print(classification_report(fin_target, fin_pred))\n",
    "        \n",
    "    return fin_target, fin_pred, np.array(fin_dia.view(-1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test_model(model, test_dl, device):\n",
    "\n",
    "    target_list = []\n",
    "    predict_list = []\n",
    "    dia_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for num, data in enumerate(tqdm(test_dl)):\n",
    "\n",
    "            data, labels, torr = data\n",
    "\n",
    "            data = data.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs, fin_stack = model(data)\n",
    "            predicted = torch.argmax(outputs.data, axis = 1)\n",
    "            \n",
    "            print(outputs, predicted)\n",
    "            sys.exit()\n",
    "\n",
    "            target_list.append(labels.cpu().numpy())\n",
    "            predict_list.append(predicted.cpu().numpy())\n",
    "                \n",
    "        fin_target = np.concatenate(target_list, axis = 0)\n",
    "        fin_pred = np.concatenate(predict_list, axis = 0)\n",
    "        \n",
    "        print(confusion_matrix(fin_target, fin_pred))\n",
    "        print(classification_report(fin_target, fin_pred))\n",
    "        \n",
    "    return fin_target, fin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da29b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fin_target, fin_pred, fin_dia = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5293b3",
   "metadata": {},
   "source": [
    "## Get TSNE graph of fault diagnosis outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed04818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "embedded = TSNE(n_components=2, learning_rate= 200).fit_transform(fin_dia)\n",
    "fin_embed = np.concatenate([embedded, fin_target.reshape(-1, 1)] ,axis = 1)\n",
    "df_embed = pd.DataFrame(fin_embed, columns = ['x', 'y', 'label'])\n",
    "\n",
    "plt.figure(figsize = [15, 10])\n",
    "sns.scatterplot(data = df_embed, hue = 'label', x = 'x', y = 'y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb4f93",
   "metadata": {},
   "source": [
    "## Get sensor importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6033142",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name == 'detection_layer.0.weight':\n",
    "        weight = param\n",
    "\n",
    "weights = weight.detach().cpu().numpy()\n",
    "weights_ = weights.reshape(11, -1)\n",
    "\n",
    "col_list = ['PM1.Gas1_Monitor', 'PM1.Gas2_Monitor', 'PM1.APC_Pressure', 'PM1.APC_Position', \n",
    "    'PM1.SourcePwr1_Read', 'PM1.SourcePwr2_Read', 'PM1.Temp1', 'PM1.Temp2',  \n",
    "    'ApcPosition', 'ApcPositionScaled','WallTemp']\n",
    "\n",
    "plt.figure(figsize = [15, 10])\n",
    "box_list = []\n",
    "for w in weights_:\n",
    "    box_list.append(w)\n",
    "    \n",
    "plt.boxplot(box_list)\n",
    "plt.xticks(range(1, 12), col_list,  rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee613db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leakage",
   "language": "python",
   "name": "leakage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
